<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: sysadmin | Core dump overflow]]></title>
  <link href="http://chousensha.github.io/blog/categories/sysadmin/atom.xml" rel="self"/>
  <link href="http://chousensha.github.io/"/>
  <updated>2018-02-10T14:12:32-05:00</updated>
  <id>http://chousensha.github.io/</id>
  <author>
    <name><![CDATA[chousensha]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[LFCS prep - Configure caching-only DNS]]></title>
    <link href="http://chousensha.github.io/blog/2018/02/10/lfcs-prep-configure-caching-only-dns/"/>
    <updated>2018-02-10T14:04:48-05:00</updated>
    <id>http://chousensha.github.io/blog/2018/02/10/lfcs-prep-configure-caching-only-dns</id>
    <content type="html"><![CDATA[<p>Today we&rsquo;ll take a look at setting up a caching only DNS server for the LFCS exam objectives.</p>

<!-- more -->


<p>First, install the necessary DNS packages: <code>yum install -y bind bind-utils</code>.</p>

<p>Then you will have to do some editing in <code>/etc/named.conf</code>. In particular, you are interested in the below:</p>

<p><code>
listen-on port 53 { 127.0.0.1; any; };
allow-query     { localhost; any; };
allow-query-cache     { localhost; any; };
</code></p>

<p>The <em>allow-query</em> option deals with who can send queries to the server, while the <em>allow-query-cache</em> allows access to cached records</p>

<p>You have to ensure the named.conf file has the proper permissions. It needs to be owned by root and belong to the group named:</p>

<p><code>
ls -l /etc/named.conf
-rw-r-----. 1 root named 1754 Feb 10 19:46 /etc/named.conf
</code></p>

<p>Check the SELinux contexts:</p>

<p><code>
ls -lZ /etc/named.*
-rw-r-----. root named unconfined_u:object_r:etc_t:s0   /etc/named.conf
-rw-r--r--. root named system_u:object_r:etc_t:s0       /etc/named.iscdlv.key
-rw-r-----. root named system_u:object_r:named_conf_t:s0 /etc/named.rfc1912.zones
-rw-r--r--. root named system_u:object_r:etc_t:s0       /etc/named.root.key
</code></p>

<p>Check the config file for syntax errors before trying anything:</p>

<p><code>
named-checkconf /etc/named.conf
</code></p>

<p>Start the DNS service:</p>

<p><code>
systemctl start named
</code></p>

<p>Open port 53 on the firewall:</p>

<p><code>
firewall-cmd --add-port=53/udp
</code></p>

<p>On the client, add the DNS server:</p>

<p><code>
nmcli con mod ens33 ipv4.dns "192.168.217.131"
</code></p>

<p>Restart the connection and NetworkManager. Check that the nameserver has been added in <code>/etc/resolv.conf</code>. Now you can test it:</p>

<p>```
nslookup github.com
Server:     192.168.217.131
Address:    192.168.217.131#53</p>

<p>Non-authoritative answer:
Name:   github.com
Address: 192.30.253.113
Name:   github.com
Address: 192.30.253.112
```</p>

<p>If you need to add a zone to your DNS server, take a look at the sample zone directives in <em>/etc/named.rfc1912.zones</em>:</p>

<p>```
zone &ldquo;localhost.localdomain&rdquo; IN {</p>

<pre><code>type master;
file "named.localhost";
allow-update { none; };
</code></pre>

<p>};
```</p>

<p>You have to add a similar configuration for your zone inside <code>/etc/named.conf</code>. Then you also have to create a zone file inside <strong>/var/named</strong>. For reference, look at an existing one:</p>

<p>```
cat /var/named/named.localhost
$TTL 1D
@   IN SOA  @ rname.invalid. (</p>

<pre><code>                0   ; serial
                1D  ; refresh
                1H  ; retry
                1W  ; expire
                3H )    ; minimum
NS  @
A   127.0.0.1
AAAA    ::1
</code></pre>

<p>```</p>

<p>This should be all that is needed in terms of DNS configuration for LFCS objectives.</p>

<p>```</p>

<hr />

<p>/ This life is yours. Some of it was \
| given to you; the rest, you made   |
\ yourself.                          /</p>

<hr />

<pre><code>    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LFCS prep - Runlevels and bootloader configuration]]></title>
    <link href="http://chousensha.github.io/blog/2018/02/08/lfcs-prep-runlevels-and-bootloader-configuration/"/>
    <updated>2018-02-08T12:47:30-05:00</updated>
    <id>http://chousensha.github.io/blog/2018/02/08/lfcs-prep-runlevels-and-bootloader-configuration</id>
    <content type="html"><![CDATA[<p>Today we&rsquo;ll look at the following LFCS objectives:</p>

<ul>
<li><p>Log into graphical and text mode consoles</p></li>
<li><p>Boot systems into different runlevels manually</p></li>
<li><p>Install, configure and troubleshoot the  bootloader</p></li>
</ul>


<!-- more -->


<h2>systemd targets</h2>

<p>First, let&rsquo;s take a look at all the available targets:</p>

<p>```
systemctl list-units &mdash;type=target &mdash;all
  UNIT                   LOAD      ACTIVE   SUB    DESCRIPTION
  basic.target           loaded    active   active Basic System
  cryptsetup.target      loaded    active   active Encrypted Volumes
  emergency.target       loaded    inactive dead   Emergency Mode
  final.target           loaded    inactive dead   Final Step
  getty.target           loaded    active   active Login Prompts
  graphical.target       loaded    active   active Graphical Interface
  local-fs-pre.target    loaded    active   active Local File Systems (Pre)
  local-fs.target        loaded    active   active Local File Systems
  multi-user.target      loaded    active   active Multi-User System
  network-online.target  loaded    active   active Network is Online
  network-pre.target     loaded    inactive dead   Network (Pre)
  network.target         loaded    active   active Network
  nfs-client.target      loaded    active   active NFS client services
  nss-lookup.target      loaded    inactive dead   Host and Network Name Lookups
  nss-user-lookup.target loaded    active   active User and Group Name Lookups
  paths.target           loaded    active   active Paths
  remote-fs-pre.target   loaded    active   active Remote File Systems (Pre)
  remote-fs.target       loaded    active   active Remote File Systems
  rescue.target          loaded    inactive dead   Rescue Mode
  rpcbind.target         loaded    inactive dead   RPC Port Mapper
  shutdown.target        loaded    inactive dead   Shutdown
  slices.target          loaded    active   active Slices
  sockets.target         loaded    active   active Sockets
  swap.target            loaded    active   active Swap
  sysinit.target         loaded    active   active System Initialization
● syslog.target          not-found inactive dead   syslog.target
  time-sync.target       loaded    inactive dead   System Time Synchronized
  timers.target          loaded    active   active Timers
  umount.target          loaded    inactive dead   Unmount All Filesystems</p>

<p>LOAD   = Reflects whether the unit definition was properly loaded.
ACTIVE = The high-level unit activation state, i.e. generalization of SUB.
SUB    = The low-level unit activation state, values depend on unit type.</p>

<p>29 loaded units listed.
To show all installed unit files use &lsquo;systemctl list-unit-files&rsquo;.
```</p>

<p>Out of these, we will only look at the most relevant ones:</p>

<ul>
<li><p><strong>emergency.target</strong> = emergency mode. All you have is systemd and a shell. This is what you use if, for instance, you experience disk failure</p></li>
<li><p><strong>rescue.target</strong> = single user mode. It&rsquo;s the equivalent of runlevel 1. In this mode, the early boot services are started and local mount points are mounted. No networking</p></li>
<li><p><strong>multi-user.target</strong> = multi-user text mode with networking. Equivalent to runlevel 3</p></li>
<li><p><strong>graphical.target</strong> = multi-user with GUI. Equivalent to runlevel 5</p></li>
</ul>


<p>Let&rsquo;s see what the default target is on my system:</p>

<p><code>
systemctl get-default
graphical.target
</code></p>

<p>You can set a new default target with <code>systemctl set-default</code></p>

<p>It is possible to switch to a different target without rebooting, by using <code>systemctl isolate <target name></code>. However, only targets with AllowIsolate=yes in their unit files can be isolated. Isolation will start or stop all necessary services for that particular target. You can use isolation to quickly switch between targets. You can achieve the same with the systemctl command:</p>

<p><code>
systemctl rescue
systemctl emergency
</code></p>

<p>Additionally, you can boot a desired target through the boot menu, by selecting <em>e</em> at the desired entry, and appending at the end of the kernel command line (the one starting with linux16) the entry <strong>systemd.unit=<target name></strong></p>

<h2>bootloader troubleshooting</h2>

<p>Here we&rsquo;re only going to look at the grub2 bootloader. Its main configuration file is <code>/boot/grub2/grub.cfg</code>, but you&rsquo;re not supposed to edit it directly, but use <code>grub2-mkconfig</code> to generate the configuration. So you first edit the GRUB parameters in <code>/etc/default/grub</code>, and then run grub2-mkconfig to create the cfg file. Let&rsquo;s see the defaults I have:</p>

<p><code>
cat /etc/default/grub
GRUB_TIMEOUT=5
GRUB_DISTRIBUTOR="$(sed 's, release .*$,,g' /etc/system-release)"
GRUB_DEFAULT=saved
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet"
GRUB_DISABLE_RECOVERY="true"
</code></p>

<p>After editing those, you can run <code>grub2-mkconfig > /boot/grub2/grub.cfg</code> to create the new boot config file.</p>

<p>If you need to reinstall the bootloader, you can do so with <code>grub2-install</code></p>

<h3>Learn more</h3>

<p><a href="https://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet">SysVinit to Systemd Cheatsheet</a></p>

<p><a href="https://lists.freedesktop.org/archives/systemd-devel/2016-February/035709.html">emergency mode vs rescue mode</a></p>

<p>```</p>

<hr />

<p>/ You will always get the greatest        \
\ recognition for the job you least like. /</p>

<hr />

<pre><code>    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LFCS prep - Using firewalld]]></title>
    <link href="http://chousensha.github.io/blog/2018/02/02/lfcs-prep-using-firewalld/"/>
    <updated>2018-02-02T14:23:40-05:00</updated>
    <id>http://chousensha.github.io/blog/2018/02/02/lfcs-prep-using-firewalld</id>
    <content type="html"><![CDATA[<p>Today we&rsquo;ll look at firewall configuration on CentOS / RedHat by using firewalld, the replacement for iptables. Since they are both mutually exclusive, if you decide to use firewalld, ensure that iptables is not running and cannot be started, by masking the service with <code>systemctl mask iptables</code></p>

<!-- more -->


<p>With firewalld, traffic is classified into zones that can have their own rules and ports / services. The default zone is called the <em>public</em> zone</p>

<p>Now let&rsquo;s see a couple of commands for accomplishing various tasks.</p>

<ul>
<li>list the predefined zones</li>
</ul>


<p><code>
firewall-cmd --get-zones
work drop internal external trusted home dmz public block
</code></p>

<ul>
<li>print the default zone</li>
</ul>


<p><code>
firewall-cmd --get-default-zone
public
</code></p>

<ul>
<li>list active zones and their interfaces</li>
</ul>


<p><code>
firewall-cmd --get-active-zones
public
  interfaces: enp0s3
</code></p>

<ul>
<li>set new default zone</li>
</ul>


<p><code>
firewall-cmd --set-default-zone=NAME
</code></p>

<ul>
<li>list predefined services</li>
</ul>


<p><code>
firewall-cmd --get-services
RH-Satellite-6 amanda-client amanda-k5-client bacula bacula-client ceph ceph-mon dhcp dhcpv6 dhcpv6-client dns docker-registry dropbox-lansync freeipa-ldap freeipa-ldaps freeipa-replication ftp high-availability http https imap imaps ipp ipp-client ipsec iscsi-target kadmin kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mosh mountd ms-wbt mysql nfs ntp openvpn pmcd pmproxy pmwebapi pmwebapis pop3 pop3s postgresql privoxy proxy-dhcp ptp pulseaudio puppetmaster radius rpc-bind rsyncd samba samba-client sane smtp smtps snmp snmptrap squid ssh synergy syslog syslog-tls telnet tftp tftp-client tinc tor-socks transmission-client vdsm vnc-server wbem-https xmpp-bosh xmpp-client xmpp-local xmpp-server
</code></p>

<ul>
<li>print information about the settings of the public zone</li>
</ul>


<p><code>
firewall-cmd --list-all --zone=public
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: enp0s3
  sources:
  services: dhcpv6-client ftp mountd nfs rpc-bind ssh
  ports:
  protocols:
  masquerade: no
  forward-ports:
  sourceports:
  icmp-blocks:
  rich rules:
</code></p>

<ul>
<li>add a service</li>
</ul>


<p><code>
firewall-cmd --add-service samba
</code></p>

<ul>
<li><p>to make changes persist through reboots, add the <strong>&mdash;permanent</strong> flag, and don&rsquo;t forget to reload after</p></li>
<li><p>add multiple services</p></li>
</ul>


<p><code>
firewall-cmd --zone=NAME --add-service={serv1,serv2,serv3}
</code></p>

<ul>
<li>add port</li>
</ul>


<p><code>
firewall-cmd add-port=8080/tcp
</code></p>

<ul>
<li>add source to a zone</li>
</ul>


<p><code>
firewall-cmd --permanent --zone=NAME--add-source=RANGE
</code></p>

<ul>
<li>add masquerade for a zone</li>
</ul>


<p><code>
firewall-cmd --zone=NAME --add-masquerade
</code></p>

<ul>
<li>port forwarding (must enable masquerade first). Forward packets destined to port 22 to port 8888</li>
</ul>


<p><code>
firewall-cmd --zone=NAME --add-forward-port=port=22:proto=tcp:toport=8888
</code></p>

<p>```</p>

<hr />

<p>/ Your best consolation is the hope that \
| the things you failed to get weren&rsquo;t   |
\ really worth having.                   /</p>

<hr />

<pre><code>    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LFCS prep - autofs automounter]]></title>
    <link href="http://chousensha.github.io/blog/2018/01/30/lfcs-prep-autofs-automounter/"/>
    <updated>2018-01-30T12:57:31-05:00</updated>
    <id>http://chousensha.github.io/blog/2018/01/30/lfcs-prep-autofs-automounter</id>
    <content type="html"><![CDATA[<p>In a <a href="http://chousensha.github.io/blog/2017/09/10/nfs-shares-on-centos-7/">previous post</a> we looked at sharing folders with NFS. Now we take a step further to look at a client-only configuration that allows on demand mounting / unmounting of various filesystems. There is no need for /etc/fstab entries and resources are preserved better. The automounter is provided by the <em>autofs</em> package. After installing it, check that the autofs service has been started, before proceeding with the configuration.</p>

<!-- more -->


<p>The main configuration is done in the master map file, located at <code>/etc/auto.master</code>. Its format is:</p>

<p><code>
&lt;mount-point&gt; &lt;map-type&gt; &lt;options&gt;
</code></p>

<ul>
<li><p><strong>mount-point</strong> = base location for the autofs filesystem to be mounted.  For indirect maps this directory will be created  (as  with  mkdir -p) and is removed when the autofs filesystem is umounted.</p></li>
<li><p><strong>map-type</strong> = map type used for this mount point. A map file can be given here</p></li>
<li><p><strong>options</strong> = mount options</p></li>
</ul>


<p>Here are the contents of my <em>/etc/auto.master</em> file:</p>

<p><code>
/mnt    /etc/auto.share
</code></p>

<p>The map file can have any name of your choosing. The auto.share file has the following format:</p>

<p><code>
&lt;mount point&gt; &lt;options&gt; &lt;location&gt;
</code></p>

<p>I made one with these values:</p>

<p><code>
nfs-share -fstype=nfs 192.168.241.130:/var/nfs-share
</code></p>

<p>The name refers to the autofs mount point. I didn&rsquo;t specify an absolute path, so the share will be mounted under the directory specified in the master map (/mnt in this case). I had a quick share served by an NFS server, and after all the above configuration, I restarted autofs and looked under /mnt:</p>

<p><code>
ls /mnt/share
docs
</code></p>

<p>Done! No need for manually adding entries to /etc/fstab and mounting them.</p>

<p>```</p>

<hr />

<p>/ Change your thoughts and you change \
\ your world.                         /</p>

<hr />

<pre><code>    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LFCS prep - LVM storage management]]></title>
    <link href="http://chousensha.github.io/blog/2018/01/27/lfcs-prep-lvm-storage-management/"/>
    <updated>2018-01-27T09:56:20-05:00</updated>
    <id>http://chousensha.github.io/blog/2018/01/27/lfcs-prep-lvm-storage-management</id>
    <content type="html"><![CDATA[<p>Today we&rsquo;ll look at logical volume management tasks. LVM improves the traditional partition to disk approach, adding flexibility (extending and reducing space), snapshot capabilities and redundancy.</p>

<!-- more -->


<p>First, some terminology is in order:</p>

<ul>
<li><p>physical volumes (PV) are the physical devices that are mapped to be used inside LVM. Their storace unit is called a physical extent (PE)</p></li>
<li><p>volume groups (VG) are storage pools made from PVs</p></li>
<li><p>logical volumes (LV) are made up of the VG space</p></li>
</ul>


<h2>Creating PVs</h2>

<p>I am now going to create 2 PVs of 100 MB each. You first need to create 2 LVM partitions. If you look inside fdisk, you see the partition code for LVM is <strong>8e</strong>:</p>

<p><code>
8e  Linux LVM
</code></p>

<p>So before writing the partition in fdisk, use <code>t</code> to change it to Linux LVM. Then verify that the new partitions were created:</p>

<p><code>
lsblk | grep sdb
sdb             8:16   0    1G  0 disk
├─sdb1          8:17   0  100M  0 part
└─sdb2          8:18   0  100M  0 part
</code></p>

<p>Now the PVs can be created:</p>

<p><code>
pvcreate /dev/sdb1 /dev/sdb2
  Physical volume "/dev/sdb1" successfully created.
  Physical volume "/dev/sdb2" successfully created.
</code></p>

<h2>Creating VGs</h2>

<p>Now you can create a VG from the 2 PVs:</p>

<p><code>
vgcreate vg-zero /dev/sdb1 /dev/sdb2
  Volume group "vg-zero" successfully created
</code></p>

<p>Let&rsquo;s look at our existing PVs now:</p>

<p><code>
pvs
  PV         VG      Fmt  Attr PSize  PFree
  /dev/sda2  rhel    lvm2 a--  29.00g  4.00m
  /dev/sdb1  vg-zero lvm2 a--  96.00m 96.00m
  /dev/sdb2  vg-zero lvm2 a--  96.00m 96.00m
</code></p>

<p>And check an individual PV in more detail:</p>

<p><code>
pvdisplay /dev/sdb1
  --- Physical volume ---
  PV Name               /dev/sdb1
  VG Name               vg-zero
  PV Size               100.00 MiB / not usable 4.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              24
  Free PE               24
  Allocated PE          0
  PV UUID               ViTFfu-jBZE-47Ie-d6qB-WTDt-fpNf-bPC00Y
</code></p>

<p>Now let&rsquo;s check our VGs:</p>

<p><code>
vgs
  VG      #PV #LV #SN Attr   VSize   VFree  
  rhel      1   2   0 wz--n-  29.00g   4.00m
  vg-zero   2   0   0 wz--n- 192.00m 192.00m
</code></p>

<p>And in more detail:</p>

<p><code>
vgdisplay vg-zero
  --- Volume group ---
  VG Name               vg-zero
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               192.00 MiB
  PE Size               4.00 MiB
  Total PE              48
  Alloc PE / Size       0 / 0   
  Free  PE / Size       48 / 192.00 MiB
  VG UUID               gUf1XI-znJt-MM2T-V721-PNbe-uxcr-m3bVTe
</code></p>

<h2>Creating LVs</h2>

<p>Next, I created a LV named inventory with a size of 128 MB, from the vg-zero VG:</p>

<p><code>
lvcreate -n inventory -L 128MB vg-zero
  Logical volume "inventory" created.
</code></p>

<p>Look at the existing LVs:</p>

<p><code>
lvs
  LV        VG      Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  root      rhel    -wi-ao----  26.99g                                                    
  swap      rhel    -wi-ao----   2.00g                                                    
  inventory vg-zero -wi-a----- 128.00m     
</code></p>

<p>And check our newly created one:</p>

<p><code>
lvdisplay /dev/vg-zero/inventory
  --- Logical volume ---
  LV Path                /dev/vg-zero/inventory
  LV Name                inventory
  VG Name                vg-zero
  LV UUID                jE4XRJ-ddlX-nM8s-NVSr-4cfK-F9O0-JcKecs
  LV Write Access        read/write
  LV Creation host, time rhel7, 2018-01-26 15:36:04 +0200
  LV Status              available
  # open                 0
  LV Size                128.00 MiB
  Current LE             32
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:2
</code></p>

<p>Before using the new LV, you need to create a filesystem on top of it. I went with XFS:</p>

<p>```
mkfs.xfs /dev/vg-zero/inventory
meta-data=/dev/vg-zero/inventory isize=512    agcount=4, agsize=8192 blks</p>

<pre><code>     =                       sectsz=512   attr=2, projid32bit=1
     =                       crc=1        finobt=0, sparse=0
</code></pre>

<p>data     =                       bsize=4096   blocks=32768, imaxpct=25</p>

<pre><code>     =                       sunit=0      swidth=0 blks
</code></pre>

<p>naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=855, version=2</p>

<pre><code>     =                       sectsz=512   sunit=0 blks, lazy-count=1
</code></pre>

<p>realtime =none                   extsz=4096   blocks=0, rtextents=0
```</p>

<p>Now prepare a location to mount it:</p>

<p><code>
mkdir /mnt/inventory
</code></p>

<p>I want it to mount at boot, so I added the following to <code>/etc/fstab</code>:</p>

<p><code>
/dev/vg-zero/inventory     /mnt/inventory                xfs    defaults        0 0
</code></p>

<p>Let&rsquo;s break down the <strong>/etc/fstab</strong> fields a bit:</p>

<ul>
<li><p>the first field represents the device, which can be specified by name, UUID, or label</p></li>
<li><p>the second field is the mount point of the device</p></li>
<li><p>the third is the filesystem type</p></li>
<li><p>fourth is for the mount options. The <em>defaults</em> option specifies the usage of default options: rw, suid, dev, exec, auto, nouser, async</p></li>
<li><p>the fifth is a backup flag for the dump utility. Use 1 to enable it and 0 to disable it</p></li>
<li><p>sixth represents the fsck automatic check at boot. Use 0 to disable it, 1 for the root filesystem, and 2 for other filesystems that might need the automatic checking</p></li>
</ul>


<p>It&rsquo;s a good practice to mount the devices you added to fstab before a reboot to ensure there are no errors: <code>mount -a</code></p>

<h2>Extending / reducing VGs</h2>

<p>To add more space to a VG, you can add more PVs to it. Let&rsquo;s see its current space:</p>

<p><code>
vgdisplay vg-zero | grep Free
  Free  PE / Size       16 / 64.00 MiB
</code></p>

<p>I created a new PV on /dev/sdb3 and added it to the VG:</p>

<p><code>
vgextend vg-zero /dev/sdb3
  Volume group "vg-zero" successfully extended
</code></p>

<p>The storage size has increased now:</p>

<p><code>
vgdisplay vg-zero | grep Free
  Free  PE / Size       28 / 112.00 MiB
</code></p>

<p>And to reduce the space of a VG, remove one or more PVs from it:</p>

<p><code>
vgreduce vg-zero /dev/sdb3
  Removed "/dev/sdb3" from volume group "vg-zero"
</code></p>

<p>To migrate the PEs used on a PV to other PVs in the VG, use <strong>pvmove</strong>:</p>

<p><code>
pvmove /dev/sdb1
  /dev/sdb1: Moved: 0.00%
  /dev/sdb1: Moved: 100.00%
</code></p>

<h2>Resizing LVs</h2>

<p>You can use <em>lvresize</em> for extending or shrinking LVs, or <em>lvextend</em> for extending only. There needs to be enough free space in the VG for growing the LV.</p>

<p><code>
lvextend -L +50M /dev/vg-zero/inventory
  Rounding size to boundary between physical extents: 52.00 MiB.
  Size of logical volume vg-zero/inventory changed from 128.00 MiB (32 extents) to 180.00 MiB (45 extents).
  Logical volume vg-zero/inventory successfully resized.
</code></p>

<p>You also need to grow the filesystem to occupy the newly extended LV. Since I picked XFS, I&rsquo;ll use <em>xfs_growfs</em> for this:</p>

<p>```
xfs_growfs /mnt/inventory/
meta-data=/dev/mapper/vg&mdash;zero-inventory isize=512    agcount=4, agsize=8192 blks</p>

<pre><code>     =                       sectsz=512   attr=2, projid32bit=1
     =                       crc=1        finobt=0 spinodes=0
</code></pre>

<p>data     =                       bsize=4096   blocks=32768, imaxpct=25</p>

<pre><code>     =                       sunit=0      swidth=0 blks
</code></pre>

<p>naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=855, version=2</p>

<pre><code>     =                       sectsz=512   sunit=0 blks, lazy-count=1
</code></pre>

<p>realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 32768 to 46080
```</p>

<p>This was just for demo, in practice you want to resize both the LV and the filesystem with a single command:</p>

<p>```
lvextend -L 190M -r /dev/vg-zero/inventory
  Rounding size to boundary between physical extents: 192.00 MiB.
  Size of logical volume vg-zero/inventory changed from 180.00 MiB (45 extents) to 192.00 MiB (48 extents).
  Logical volume vg-zero/inventory successfully resized.
meta-data=/dev/mapper/vg&mdash;zero-inventory isize=512    agcount=6, agsize=8192 blks</p>

<pre><code>     =                       sectsz=512   attr=2, projid32bit=1
     =                       crc=1        finobt=0 spinodes=0
</code></pre>

<p>data     =                       bsize=4096   blocks=46080, imaxpct=25</p>

<pre><code>     =                       sunit=0      swidth=0 blks
</code></pre>

<p>naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal               bsize=4096   blocks=855, version=2</p>

<pre><code>     =                       sectsz=512   sunit=0 blks, lazy-count=1
</code></pre>

<p>realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 46080 to 49152
```</p>

<p>This time I specified an absolute size instead of adding a certain amount.</p>

<h2>Removing LVs</h2>

<p>When you&rsquo;re done, you backed up your data or no longer need it, you can delete the LV. Unmount it first:</p>

<p><code>
umount -v /mnt/inventory/
umount: /mnt/inventory (/dev/mapper/vg--zero-inventory) unmounted
</code></p>

<p>Remove the LV:</p>

<p><code>
lvremove /dev/vg-zero/inventory
Do you really want to remove active logical volume vg-zero/inventory? [y/n]: y
  Logical volume "inventory" successfully removed
</code></p>

<p>Remove the VG:</p>

<p><code>
vgremove vg-zero
  Volume group "vg-zero" successfully removed
</code></p>

<p>Finally, also remove the PVs:</p>

<p><code>
pvremove /dev/sdb1 /dev/sdb2
  Labels on physical volume "/dev/sdb1" successfully wiped.
  Labels on physical volume "/dev/sdb2" successfully wiped.
</code></p>

<p>And don&rsquo;t forget to delete any remaining fstab entries.</p>

<p>```</p>

<hr />

<p>/ That secret you&rsquo;ve been guarding, \
\ isn&rsquo;t.                            /</p>

<hr />

<pre><code>    \   ^__^
     \  (oo)\_______
        (__)\       )\/\
            ||----w |
            ||     ||
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
</feed>
